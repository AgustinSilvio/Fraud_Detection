{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112c23c4",
   "metadata": {},
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b4dd2",
   "metadata": {},
   "source": [
    "1. [Introducción a Random Forest](#introducción-a-random-forest)\n",
    "\n",
    "    1.0. [¿Qué es Random Forest?](#qué-es-random-forest)\n",
    "    1.1. [Fundamentos teóricos](#fundamentos-teóricos)\n",
    "    1.2. [Proceso de predicción: Votación y Promediado](#proceso-de-predicción-votación-y-promediado)\n",
    "    1.3. [Ventajas y Desventajas](#ventajas-y-desventajas)\n",
    "\n",
    "2. [Implementación del modelo](#implementación-del-modelo)\n",
    "   2.1. [Packages](#packages)\n",
    "\n",
    "   2.3. [Balancear mejor la muestra](#balancear-mejor-la-muestra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b2051",
   "metadata": {},
   "source": [
    "# Introducción a Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bbf60",
   "metadata": {},
   "source": [
    "## ¿Qué es Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f183f",
   "metadata": {},
   "source": [
    "Dentro del vasto ecosistema de algoritmos de machine learning, el algoritmo Random Forest (o Bosque Aleatorio) se ha consolidado como una de las herramientas más poderosas y versátiles. Pertenece a la categoría de métodos de aprendizaje de conjunto (ensemble learning), que combinan las predicciones de múltiples modelos para producir un resultado más preciso y robusto que el de cualquier modelo individual.\n",
    "\n",
    "Random Forest se construye a partir de una multitud de árboles de decisión, que son en sí mismos un tipo de algoritmo de aprendizaje supervisado. La metáfora del \"bosque\" es apropiada: así como un bosque está compuesto por muchos árboles, este algoritmo combina las predicciones de muchos árboles de decisión individuales para formar una predicción colectiva más fuerte y confiable.\n",
    "\n",
    "Los árboles de decisión son algoritmos de aprendizaje supervisado que particionan recursivamente el espacio de características en regiones más pequeñas, asignando una etiqueta de clase o valor a cada región. Aunque son intuitivos y fáciles de interpretar, los árboles individuales tienden a sufrir de overfitting, especialmente con datos complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f173acc",
   "metadata": {},
   "source": [
    "## Fundamentos teóricos: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab3b3e",
   "metadata": {},
   "source": [
    "**Bagging y Aleatoriedad de Características**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616cdfd7",
   "metadata": {},
   "source": [
    "El poder de Random Forest reside en dos principios fundamentales.\n",
    "\n",
    "- **Bagging (Bootstrap Aggregation)**: Introducido por Leo Breiman en 1996. Este método consiste en crear  múltiples subconjuntos de datos de entrenamiento mediante muestreo aleatorias con reemplazodel conjunto de datos original. Cada árbol se entrena con uno de estos subconjuntos.\n",
    "- **Aleatoriedad de las Características**: En cada división de nodo, se considera solo un subconjunto aleatorio de características. Esto reduce la correlación entre los árboles. Cada subconjunto puede contener algunas observaciones repetidas y otras ausentes, lo que garantiza que cada árbol individual se entrene con una perspectiva ligeramente diferente de los datos. Al final, las predicciones de todos los árboles se combinan mediante votación mayoritaria (clasificación) o promediado (regresión)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06290a49",
   "metadata": {},
   "source": [
    "## Proceso de predicción: Votación y Promediado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbce231",
   "metadata": {},
   "source": [
    "Para evitar que los árboles individuales se correlacionen demasiado (lo que ocurriría si todos los árboles se dividieran en las mismas características dominantes), Random Forest introduce una segunda capa de aleatoriedad. En cada nodo de cada árbol, el algoritmo no considera todas las características disponibles, sino que selecciona al azar un subconjunto de ellas para encontrar la mejor división.\n",
    "\n",
    "- **Clasificación**: Cada árbol vota por una clase y la clase mayoritaria se selecciona como predicción.\n",
    "- \n",
    "- **Regresión**: Se promedian las predicciones de todos los árboles.\n",
    "\n",
    "Esta doble aleatorización (en las muestras y en las características) es lo que hace que Random Forest sea tan efectivo para reducir la varianza y mejorar la generalización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f4ef6",
   "metadata": {},
   "source": [
    "## Ventajas y Desventajas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320f714",
   "metadata": {},
   "source": [
    "| Ventajas | Desventajas |\n",
    "|----------|-------------|\n",
    "| Alta precisión | Complejidad computacional |\n",
    "| Robustez ante ruido y valores atípicos | Requiere más recursos |\n",
    "| Maneja datos de alta dimensión | Menor interpretabilidad (\"caja negra\") |\n",
    "| Flexibilidad (clasificación y regresión) | Período de entrenamiento largo |\n",
    "| No requiere escalamiento de características | Puede tender a sobreajustar si no se sintoniza |\n",
    "| Puede manejar valores faltantes | Tiende a tener un sesgo hacia variables con muchos valores únicos |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c36c7",
   "metadata": {},
   "source": [
    "# Implementación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4499a67",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12932bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from mlparadetectarfraudes.data import data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca39ab",
   "metadata": {},
   "source": [
    "# Importar datos estandarizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_estandarizados = pd.read_csv(data_interim_dir(\"data_estandarizados.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b11380",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Importancia de características\n",
    "feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "feature_importances.nlargest(10).plot(kind='barh')\n",
    "plt.title('Top 10 Importancia de Características')\n",
    "plt.show()\n",
    "# Visualización de un árbol individual (requiere graphviz)\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(rf_model.estimators_[0], feature_names=X.columns, filled=True, rounded=True, max_depth=3)\n",
    "plt.title(\"Ejemplo de Árbol de Decisión en el Bosque\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### . Optimización de Hiperparámetros\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Definir la grilla de parámetros\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "# Búsqueda en grilla\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Mejores parámetros\n",
    "print(\"Mejores parámetros:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05d7e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1592aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features y target\n",
    "x = data.drop('is_fraud', axis=1)\n",
    "y = data['is_fraud']\n",
    "\n",
    "# Balancear datos con SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.3, random_state=42)\n",
    "x_res, y_res = smote.fit_resample(x, y)\n",
    "\n",
    "# Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_res, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
    "\n",
    "# Entrenamiento\n",
    "model_RF = RandomForestClassifier(\n",
    "    n_estimators=100,    # Número de árboles en el bosque\n",
    "    criterion='gini',    # Función para medir la calidad de la división\n",
    "    max_depth=100,      # Profundidad máxima de los árboles\n",
    "    min_samples_split=2, # Mínimo número de muestras requeridas para dividir un nodo interno\n",
    "    min_samples_leaf=1,  # Mínimo número de muestras requeridas en un nodo hoja\n",
    "    max_features='auto', # Número de características a considerar para buscar la mejor división\n",
    "    bootstrap=True,      # Whether to use bootstrap samples when building trees\n",
    "    random_state=42,     # Semilla para reproducibilidad\n",
    "    class_weight='balanced',  # Ajustar pesos para clases desbalanceadas\n",
    "    n_jobs=1               # Número de trabajos a ejecutar en paralelo\n",
    ")\n",
    "model_RF.fit(x_train, y_train)\n",
    "\n",
    "# Evaluación\n",
    "y_pred = model_RF.predict(x_test)\n",
    "y_pred_proba = model_RF.predict_proba(x_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Precisión del modelo: {accuracy:.4f}\")\n",
    "print('Reporte de clasificación')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Random Forest - Rendimiento:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "# Matriz de confusión\n",
    "print('Matriz de confusión')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confusión - Random Forest')\n",
    "plt.ylabel('Verdaderos')\n",
    "plt.xlabel('Predichos')\n",
    "plt.show()\n",
    "\n",
    "# Asegúrate de que la carpeta exista\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Guarda los archivos en la carpeta models\n",
    "joblib.dump(model_RF, '../models/model_fraude_RF.pkl')\n",
    "joblib.dump(scaler, '../models/scaler.pkl')  # Save the scaler as well\n",
    "\n",
    "# Función de predicción\n",
    "def predict_fraude(transaction_data: dict, model):\n",
    "    # Create DataFrame from input\n",
    "    transaction_data_df = pd.DataFrame(transaction_data, index=[0])\n",
    "\n",
    "    # Scale numerical features\n",
    "    transaction_data_df[['amount', 'user_age']] = scaler.transform(transaction_data_df[['amount', 'user_age']])\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    transaction_data_df = pd.get_dummies(transaction_data_df)\n",
    "\n",
    "    # Ensure all columns are present\n",
    "    for col in x_train.columns:\n",
    "        if col not in transaction_data_df.columns:\n",
    "            transaction_data_df[col] = 0\n",
    "\n",
    "    # Reorder columns to match training data\n",
    "    transaction_data_df = transaction_data_df[x_train.columns]\n",
    "\n",
    "    prob = model.predict_proba(transaction_data_df)[0][1]  # Fixed typo: predict_prob to predict_proba\n",
    "    return {'fraud_probability': round(prob, 4), 'is_fraud': prob >= 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Entrenar\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predecir sobre registro nuevo\n",
    "new_tx = {'amount': 123.45, 'user_age': 35, 'country': 'US', …}\n",
    "new_df = pd.DataFrame([new_tx])\n",
    "\n",
    "prob = pipeline.predict_proba(new_df)[0, 1]   # Probabilidad de fraude\n",
    "label = pipeline.predict(new_df)[0]           # 0 o 1\n",
    "\n",
    "print({'prob_fraude': prob, 'alertra': bool(label)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89768ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretación del modelo: Importancia de características\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Obtener la importancia de las características\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualizar las características más importantes\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances.head(15))\n",
    "plt.title('Top 15 Características Más Importantes para Detección de Fraudes')\n",
    "plt.xlabel('Importancia')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de las probabilidades predichas\n",
    "fraud_probas = y_pred_proba[y_test == 1]  # Probabilidades para casos de fraude reales\n",
    "non_fraud_probas = y_pred_proba[y_test == 0]  # Probabilidades para casos normales\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(non_fraud_probas, bins=50, alpha=0.7, label='Transacciones Normales')\n",
    "plt.hist(fraud_probas, bins=50, alpha=0.7, label='Fraudes Reales')\n",
    "plt.xlabel('Probabilidad Predicha de Fraude')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribución de Probabilidades Predichas')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6243112",
   "metadata": {},
   "source": [
    "## Optimización de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26faef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimización de hiperparámetros con GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Definir la métrica de evaluación (F1-score es importante para datos desbalanceados)\n",
    "scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Definir la grilla de parámetros a probar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en grilla\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "    param_grid=param_grid,\n",
    "    scoring=scorer,\n",
    "    cv=5,           # Validación cruzada de 5 folds\n",
    "    n_jobs=-1,      # Usar todos los procesadores disponibles\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Ejecutar la búsqueda (puede tomar tiempo)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejores parámetros encontrados\n",
    "print(\"Mejores parámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación F1:\", grid_search.best_score_)\n",
    "\n",
    "# Entrenar el modelo final con los mejores parámetros\n",
    "best_rf_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6186ecfa",
   "metadata": {},
   "source": [
    "# Balancear mejor la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Opción 1: Usar SMOTE para oversampling de la clase minoritaria\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Distribución después de SMOTE:\")\n",
    "print(pd.Series(y_resampled).value_counts())\n",
    "\n",
    "# Opción 2: Combinar oversampling y undersampling\n",
    "resampling_pipeline = ImbPipeline([\n",
    "    ('oversample', SMOTE(sampling_strategy=0.1, random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(sampling_strategy=0.5, random_state=42))\n",
    "])\n",
    "\n",
    "X_resampled, y_resampled = resampling_pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "# Entrenar Random Forest con datos balanceados\n",
    "rf_balanced = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # Esto da más peso a la clase minoritaria\n",
    ")\n",
    "rf_balanced.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predecir y evaluar\n",
    "y_pred_balanced = rf_balanced.predict(X_test)\n",
    "y_pred_proba_balanced = rf_balanced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest Balanceado - Rendimiento:\")\n",
    "print(classification_report(y_test, y_pred_balanced))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba_balanced):.4f}\")\n",
    "\n",
    "# Matriz de confusión para Random Forest balanceado\n",
    "cm_rf_balanced = confusion_matrix(y_test, y_pred_balanced)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_rf_balanced, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Fraude', 'Fraude'],\n",
    "            yticklabels=['No Fraude', 'Fraude'])\n",
    "plt.title('Matriz de Confusión - Random Forest Balanceado')\n",
    "plt.ylabel('Verdaderos')\n",
    "plt.xlabel('Predichos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4bfaf",
   "metadata": {},
   "source": [
    "# Model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data.drop('is_fraud', axis=1)\n",
    "y = data['is_fraud']\n",
    "\n",
    "# Identify column types\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), #Reemplazar valores faltantes con la mediana\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), #Reemplazar valores faltantes con  los valores más comunes\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first')) # Use variables dummies\n",
    "])\n",
    "\n",
    "# Combine preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Example of how the final pipeline will look\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(sampling_strategy=0.3, random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlparadetectarfraudes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
