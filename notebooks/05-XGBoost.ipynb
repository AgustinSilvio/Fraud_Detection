{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2379b139",
   "metadata": {},
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb413662",
   "metadata": {},
   "source": [
    "1. [Introducción a XGBoost](#introducción-a-xgboost)\n",
    "\n",
    "   1.1. [Principales características y ventajas](#principales-características-y-ventajas)\n",
    "   \n",
    "      1.1.0 [Diferencias entre XGBoost y otros algoritmos de ensemble](#diferencias-entre-xgboost-y-otros-algoritmos-de-ensemble)\n",
    "\n",
    "   1.2. [Matemática detrás de XGBoost](#matemática-detrás-de-xgboost)\n",
    "\n",
    "      1.2.0 [Ajuste de hiperparámetros](#ajuste-de-hiperparámetros)\n",
    "\n",
    "2. [Implementación del modelo](#implementación-del-modelo)\n",
    "\n",
    "   2.0 [Paquetes](#paquetes)\n",
    "   2.1 [Modelo](#modelo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b51ca5",
   "metadata": {},
   "source": [
    "# Introducción a XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b16aa7",
   "metadata": {},
   "source": [
    "XGBoost (eXtreme Gradient Boosting) es un algoritmo de machine learning creado por Tianqi Chen. Es una implementación optimizada del algoritmo de Gradient Boosting.\n",
    "\n",
    "XGBoost es un algoritmo de ensamble que construye modelos de forma secuencial, donde cada nuevo modelo intenta corregir los errores de los modelos anteriores. Utiliza el principio de gradient boosting, que optimiza una función de pérdida mediante el descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd43d13",
   "metadata": {},
   "source": [
    "## Principales características y ventajas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f195c",
   "metadata": {},
   "source": [
    "- **Alto rendimiento**: Suele ofrecer mejores resultados que otros algoritmos en una variedad de datasets.\n",
    "  \n",
    "- **Velocidad y eficiencia**: Optimizado para ser rápido y eficiente en el uso de recursos.\n",
    "  \n",
    "- **Regularización**: Incluye regularización L1 (Lasso) y L2 (Ridge) que ayuda a prevenir el sobreajuste.\n",
    "  \n",
    "- **Manejo de valores faltantes**: Puede manejar automáticamente valores faltantes en los datos.\n",
    "  \n",
    "- **Flexibilidad**: Soporta múltiples funciones de pérdida y permite la personalización.\n",
    "  \n",
    "- **Parallelismo**: Aprovecha el paralelismo en la construcción de árboles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79319fe8",
   "metadata": {},
   "source": [
    "### Diferencias entre XGBoost y otros algoritmos de ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d984f43",
   "metadata": {},
   "source": [
    "- **Random Forest**: Construye árboles de forma independiente y luego promedia (bagging), mientras que XGBoost construye árboles de forma secuencial (boosting).\n",
    "  \n",
    "- **Gradient Boosting tradicional**: XGBoost es una implementación más eficiente y escalable, con regularización y manejo de missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f42e3a",
   "metadata": {},
   "source": [
    "## Matemática detrás de XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c926c2",
   "metadata": {},
   "source": [
    "XGBoost optimiza una función de pérdida regularizada. El objetivo en cada paso es:\n",
    "```\n",
    "Obj = Σ L(y_i, y_hat_i) + Σ Ω(f_k)\n",
    "```\n",
    "donde:\n",
    "- `L` es la función de pérdida (ej: log loss para clasificación)\n",
    "- `Ω` es el término de regularización que penaliza la complejidad del modelo\n",
    "- `f_k` es el árbol en la iteración k\n",
    "El algoritmo utiliza gradientes de segundo orden (Hessianos) para optimizar la función de pérdida, lo que acelera la convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d14929",
   "metadata": {},
   "source": [
    "### Ajuste de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d831f7",
   "metadata": {},
   "source": [
    "Los hiperparámetros más importantes en XGBoost son:\n",
    "\n",
    "- `n_estimators`: Número de árboles.\n",
    "  \n",
    "- `max_depth`: Profundidad máxima de los árboles.\n",
    "  \n",
    "- `learning_rate`: Tasa de aprendizaje (shrinkage).\n",
    "  \n",
    "- `subsample`: Fracción de muestras usadas para entrenar cada árbol.\n",
    "  \n",
    "- `colsample_bytree`: Fracción de características usadas para cada árbol.\n",
    "  \n",
    "- `scale_pos_weight`: Controla el balance de clases.\n",
    "  \n",
    "- `reg_alpha` (alpha): Regularización L1.\n",
    "  \n",
    "- `reg_lambda` (lambda): Regularización L2.\n",
    "Se pueden ajustar con técnicas como GridSearchCV o RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aece5c",
   "metadata": {},
   "source": [
    "# Implementación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a808257",
   "metadata": {},
   "source": [
    "## Paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053fe849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487765c9",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=np.sum(y == 0) / np.sum(y == 1)  # Balancear clases\n",
    ")\n",
    "xgb_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predecir y evaluar\n",
    "y_pred_xgb = xgb_model.predict(X_test_selected)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "print(\"XGBoost - Rendimiento:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba_xgb):.4f}\")\n",
    "\n",
    "# Comparar curvas ROC\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_score(y_test, y_pred_proba):.4f})')\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {roc_auc_score(y_test, y_pred_proba_xgb):.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curvas ROC - Comparación de Modelos')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Matriz de confusión para XGBoost\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Fraude', 'Fraude'],\n",
    "            yticklabels=['No Fraude', 'Fraude'])\n",
    "plt.title('Matriz de Confusión - XGBoost')\n",
    "plt.ylabel('Verdaderos')\n",
    "plt.xlabel('Predichos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importancia de características\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualizar\n",
    "xgb.plot_importance(model)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlparadetectarfraudes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
